{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29240d4f",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding\n",
    "Recall: The purpose of tokenization is to break up a section of text into repeated, manageable pieces with a relatively consistent meaning\n",
    "* root words, prefixes, suffixes, etc.\n",
    "\n",
    "Last class, we talked about Byte-Pair Encoding, which repeatedly goes through a sample of text, merging the most common pairs of characters/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5689655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hug hug hug hug hug pug pug pug pun pun pun pun pun pun bun bun hugs hugs hugs\n"
     ]
    }
   ],
   "source": [
    "# setting up an input string\n",
    "word_counts = [(\"hug\", 5), (\"pug\", 3), (\"pun\", 6), (\"bun\", 2), (\"hugs\", 3)]\n",
    "input_text = \"\"\n",
    "for word, count in word_counts:\n",
    "    input_text += f\"{word} \" * count\n",
    "\n",
    "input_text = input_text[:-1] # remove trailing whitespace\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2734dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hug', 'Ġhug', 'Ġhug', 'Ġhug', 'Ġhug', 'Ġp', 'ug', 'Ġp', 'ug', 'Ġp', 'ug', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġ', 'b', 'un', 'Ġ', 'b', 'un', 'Ġhug', 's', 'Ġhug', 's', 'Ġhug', 's']\n"
     ]
    }
   ],
   "source": [
    "from bytepairencoding import BytePairEncoder\n",
    "from pretoken import Pretoken\n",
    "\n",
    "bpe = BytePairEncoder()\n",
    "\n",
    "bpe.train(input_text, 14)\n",
    "tokenized_text = bpe.tokenize(input_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2940048",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "Recall from last class: \n",
    "* First, we split the text into pretokens (often on whitespace and punctuation)\n",
    "* then, we split those pretokens into characters\n",
    "* finally, we merge consecutive tokens according to our vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affd2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "pre_tokens = [\"pun\",\"bug\",\"hugs\",\"mug\"]\n",
    "split_pre_tokens = [[\"p\",\"u\",\"n\"],[\"b\",\"u\",\"g\"],[\"h\",\"u\",\"g\",\"s\"],[\"m\",\"u\",\"g\"]]\n",
    "split_pre_tokens = [[\"p\",\"u\",\"n\"],[\"b\",\"u\",\"g\"],[\"h\",\"u\",\"g\",\"s\"],[\"[UNK]\",\"u\",\"g\"]] #there is no m in the vocabulary\n",
    "after_first_merge_rule = [[\"p\",\"u\",\"n\"],[\"b\",\"ug\"],[\"h\",\"ug\",\"s\"],[\"[UNK]\",\"ug\"]]\n",
    "after_second_merge_rule = [[\"p\",\"un\"],[\"b\",\"ug\"],[\"h\",\"ug\",\"s\"],[\"[UNK]\",\"ug\"]]\n",
    "after_third_merge_rule = [[\"p\",\"un\"],[\"b\",\"ug\"],[\"hug\",\"s\"],[\"[UNK]\",\"ug\"]] \n",
    "final_tokens = [\"p\",\"un\",\"b\",\"ug\",\"hug\",\"s\",\"[UNK]\",\"ug\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2a081",
   "metadata": {},
   "source": [
    "Pretokenizer iterates through the input text, collapsing whitespace and converting punctuation to a standalone token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ddb7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h -> u -> g', 'Ġ -> h -> u -> g', 'Ġ -> h -> u -> g', 'Ġ -> h -> u -> g', 'Ġ -> h -> u -> g', 'Ġ -> p -> u -> g', 'Ġ -> p -> u -> g', 'Ġ -> p -> u -> g', 'Ġ -> p -> u -> n', 'Ġ -> p -> u -> n', 'Ġ -> p -> u -> n', 'Ġ -> p -> u -> n', 'Ġ -> p -> u -> n', 'Ġ -> p -> u -> n', 'Ġ -> b -> u -> n', 'Ġ -> b -> u -> n', 'Ġ -> h -> u -> g -> s', 'Ġ -> h -> u -> g -> s', 'Ġ -> h -> u -> g -> s']\n"
     ]
    }
   ],
   "source": [
    "# see:\n",
    "BytePairEncoder._pretokenize\n",
    "\n",
    "print(bpe._pretokenize(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195ec37",
   "metadata": {},
   "source": [
    "Then, we iterate through the pretokens applying our merge rules (vocabulary). \n",
    "\n",
    "One key difference here is that the pretokens aren't actually strings - think of them more like linked lists; when applying a merge rule, we're really just combining two consecutive nodes into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9b42db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:\n",
      "['s', 'n', 'h', 'Ġ', 'b', 'u', 'p', 'g', 'ug', 'Ġp', 'un', 'hug', 'Ġhug', 'Ġpun']\n",
      "tokenized text:\n",
      "['hug', 'Ġhug', 'Ġhug', 'Ġhug', 'Ġhug', 'Ġp', 'ug', 'Ġp', 'ug', 'Ġp', 'ug', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġpun', 'Ġ', 'b', 'un', 'Ġ', 'b', 'un', 'Ġhug', 's', 'Ġhug', 's', 'Ġhug', 's']\n"
     ]
    }
   ],
   "source": [
    "# see: \n",
    "BytePairEncoder.tokenize\n",
    "Pretoken.apply_merge_rule\n",
    "\n",
    "print(\"vocabulary:\")\n",
    "print(bpe.vocabulary)\n",
    "\n",
    "print(\"tokenized text:\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa2782",
   "metadata": {},
   "source": [
    "## What about training?\n",
    "First, we generate a corpus of tokens and their frequencies and set the initial vocabulary of unique characters. \n",
    "\n",
    "Then, we iterate through the list of tokens, finding the most common pair of tokens, merging them, and adding to the vocabulary. Rinse and repeat until we either a) hit the vocabulary limit; or b) run out of tokens to merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9588b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ġ -> p -> u -> n', 6), ('Ġ -> h -> u -> g', 4), ('Ġ -> p -> u -> g', 3), ('Ġ -> h -> u -> g -> s', 3), ('Ġ -> b -> u -> n', 2), ('h -> u -> g', 1)]\n"
     ]
    }
   ],
   "source": [
    "# see: \n",
    "BytePairEncoder.train\n",
    "\n",
    "pt = bpe._pretokenize(input_text)\n",
    "print(bpe._generate_token_corpus(pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d28a6",
   "metadata": {},
   "source": [
    "## Testing on a larger sample of text\n",
    "Here we'll fetch the text of Sherlock Holmes we used towards the end of class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d21ba84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://www.gutenberg.org/files/1661/1661-0.txt\")\n",
    "sherlock_raw_text = response.text\n",
    "\n",
    "bpe2 = BytePairEncoder()\n",
    "bpe2.train(sherlock_raw_text, 3_000)\n",
    "\n",
    "# leave out first 900 characters of copyright stuff\n",
    "tokenized_sherlock = bpe2.tokenize(sherlock_raw_text[900:10_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd302b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last few tokens in the vocabulary\n",
      "['reck', 'Ġris', 'Ġtrif', 'yal', 'Ġprodu', 'urely', 'clo', 'sive', 'rig', 'ges', 'aff', 'ously', 'Ġmag', 'vering', 'atic', 'Ġ“‘D', 'osed', 'Ġfla', 'Ġpers', 'Ġder']\n",
      "tokenizing some text\n",
      "['ĠTo', 'ĠSherlock', 'ĠHolmes', 'Ġshe', 'Ġis', 'Ġalways', 'Ġ', '_', 'the', '_', 'Ġwoman', '.', 'ĠI', 'Ġhave', 'Ġse', 'ld', 'om', 'Ġheard', 'Ġhim', 'Ġm', 'ention', 'Ġher', 'Ġunder', 'Ġany', 'Ġother', 'Ġname', '.', 'ĠIn', 'Ġhis', 'Ġeyes', 'Ġshe', 'Ġe', 'c', 'li', 'ps', 'es', 'Ġand', 'Ġp', 'red', 'om', 'in', 'ates', 'Ġthe', 'Ġwhole', 'Ġof', 'Ġher', 'Ġse', 'x', '.', 'ĠIt', 'Ġwas', 'Ġnot', 'Ġthat', 'Ġhe', 'Ġfelt', 'Ġany', 'Ġem', 'ot', 'ion', 'Ġa', 'k', 'in', 'Ġto', 'Ġlove', 'Ġfor', 'ĠIrene', 'ĠAdler', '.', 'ĠAll', 'Ġem', 'ot', 'ions', ',', 'Ġand', 'Ġthat', 'Ġone', 'Ġparticular', 'ly', ',', 'Ġwere', 'Ġab', 'h', 'or', 're', 'nt', 'Ġto', 'Ġhis', 'Ġcold', ',', 'Ġpre', 'cise', 'Ġbut', 'Ġadmir', 'ably', 'Ġb', 'al', 'anced', 'Ġmind', '.', 'ĠHe', 'Ġwas', ',', 'ĠI', 'Ġtake', 'Ġit', ',', 'Ġthe', 'Ġmost', 'Ġperfect', 'Ġreasoning', 'Ġand', 'Ġobser', 'ving', 'Ġmachine', 'Ġthat', 'Ġthe', 'Ġworld', 'Ġhas', 'Ġseen', ',', 'Ġbut', 'Ġas', 'Ġa', 'Ġlo', 'ver', 'Ġhe', 'Ġwould', 'Ġhave', 'Ġplaced', 'Ġhimself', 'Ġin', 'Ġa', 'Ġf', 'al', 'se', 'Ġposition', '.', 'ĠHe', 'Ġnever', 'Ġspoke', 'Ġof', 'Ġthe', 'Ġso', 'f', 'ter', 'Ġpass', 'ions', ',', 'Ġsave', 'Ġwith', 'Ġa', 'Ġgi', 'be', 'Ġand', 'Ġa', 'Ġs', 'ne', 'er', '.', 'ĠThey', 'Ġwere', 'Ġadmir', 'able', 'Ġthings', 'Ġfor', 'Ġthe', 'Ġobser', 'ver', '—', 'ex', 'ce', 'llent', 'Ġfor', 'Ġdrawing', 'Ġthe', 'Ġve', 'il', 'Ġfrom', 'Ġmen', '’s', 'Ġmot', 'ives', 'Ġand', 'Ġact', 'ions', '.', 'ĠBut', 'Ġfor', 'Ġthe', 'Ġtra', 'ined', 'Ġreason', 'er', 'Ġto', 'Ġadm', 'it', 'Ġsuch', 'Ġint', 'r', 'us', 'ions', 'Ġinto', 'Ġhis', 'Ġown', 'Ġdelic', 'ate', 'Ġand', 'Ġfine', 'ly', 'Ġad', 'j', 'ust', 'ed', 'Ġtem', 'per', 'am', 'ent', 'Ġwas', 'Ġto', 'Ġintro', 'du', 'ce', 'Ġa', 'Ġdist', 'ract', 'ing', 'Ġfact', 'or', 'Ġwhich', 'Ġmight', 'Ġthrow', 'Ġa', 'Ġdoubt', 'Ġupon', 'Ġall', 'Ġhis', 'Ġm', 'ent', 'al', 'Ġresults', '.', 'ĠGr', 'it', 'Ġin', 'Ġa', 'Ġsens', 'it', 'ive', 'Ġinst', 'r', 'um', 'ent', ',', 'Ġor', 'Ġa', 'Ġcra', 'ck', 'Ġin', 'Ġone', 'Ġof', 'Ġhis', 'Ġown', 'Ġhigh', '-', 'p', 'ow', 'er', 'Ġlens', 'es', ',', 'Ġwould', 'Ġnot', 'Ġbe', 'Ġmore', 'Ġdist', 'urb', 'ing', 'Ġthan', 'Ġa', 'Ġstrong', 'Ġem', 'ot', 'ion', 'Ġin', 'Ġa', 'Ġnature', 'Ġsuch', 'Ġas', 'Ġhis', '.', 'ĠAnd', 'Ġyet', 'Ġthere', 'Ġwas', 'Ġbut', 'Ġone', 'Ġwoman', 'Ġto', 'Ġhim', ',']\n"
     ]
    }
   ],
   "source": [
    "print(\"last few tokens in the vocabulary\")\n",
    "print(bpe2.vocabulary[-20:])\n",
    "\n",
    "print(\"tokenizing some text\")\n",
    "# and leave out the table of contents and such\n",
    "print(tokenized_sherlock[200:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
