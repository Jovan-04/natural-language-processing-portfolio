{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb28e14",
   "metadata": {},
   "source": [
    "## Applied Exploration\n",
    "\n",
    "Come up with a prompt to use in a model comparison and prompt sensitivity experiment. Something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d825f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_test1 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a world-class chef.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the Maillard reaction to a first-year student in culinary school.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7b2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_test2 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert computer science professor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the Maillard reaction to a first-year student in culinary school.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e3ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_test3 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a world-class chef.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the Maillard reaction to a fourth-year student in culinary school.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72784cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTDIR = r\"./F2_LM_Responses\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4560ff",
   "metadata": {},
   "source": [
    "Then, create two additional slight variations, one in the system prompt (e.g., *expert professor* instead of *helpful assistant*) and one in the user prompt.\n",
    "\n",
    "Run each of the three variations using the [gpt-5.2 model](https://developers.openai.com/api/docs/models/gpt-5.2) (you can use web search if you want) three times and record all nine responses. \n",
    "\n",
    "Answer the following questions:\n",
    "* When you repeated the request on the same prompt, how different were the responses?\n",
    "* Were there any meaningful differences in the variations of the prompt you tried or was it similar to the differences you noticed in on repetitions of the same prompt?\n",
    "* What changes seem to be the most meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1198eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# import API key; I know there's a library for this, but that's not really necessary here\n",
    "with open(\"../.env\") as envfile:\n",
    "    env = {key: val for key, val in map(lambda l: l.split('=', 1), envfile.read().splitlines())}\n",
    "\n",
    "client = OpenAI(api_key=env['OPENAI_API_KEY'])\n",
    "\n",
    "for test, n in zip([chat_test1, chat_test2, chat_test3], range(1, 4)):\n",
    "    with open(f'{OUTPUTDIR}/gpt_test{n}.txt', 'w') as outfile:\n",
    "        outfile.write(f\"Responses to test {n} with GPT 5.2\\n\")\n",
    "        for _ in range(3):\n",
    "            outfile.write(f\"{'=' * 40} \\n\\n\")\n",
    "            gpt_response = client.responses.create(\n",
    "                model=\"gpt-5.2\",\n",
    "                input=test\n",
    "            )\n",
    "            outfile.write(gpt_response.output_text + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2091d",
   "metadata": {},
   "source": [
    "### When you repeated the request on the same prompt, how different were the responses?\n",
    "* not much difference, honestly\n",
    "* the words are different, and some of the points are in different orders, but the information was largely the same\n",
    "    * what the Maillard reaction is\n",
    "    * how it happens/how to control it\n",
    "    * common misconceptions\n",
    "\n",
    "### Were there any meaningful differences in the variations of the prompt you tried or was it similar to the differences you noticed in on repetitions of the same prompt?\n",
    "* the change in system prompt (world-class chef vs. expert computer scientist) didn't seem to have any effect\n",
    "* the change in user prompt (first-year vs. fourth-year culinary student) did seem to have a minor effect\n",
    "    * went into slightly more detail, with a longer response\n",
    "    * talked about specific chemical reactions that were happening (e.g. reducing sugars reacting with amino groups)\n",
    "\n",
    "### What changes seem to be the most meaningful?\n",
    "* changing the user prompt seems much more meaningful than changing the system prompt, at least for this example\n",
    "* I imagine the user prompt (changing the role of the chatbot) might matter more for more in-depth domain-specific knowledge\n",
    "* I wonder if the chatbot has been trained to ignore the system prompt to some extent in favor of the user prompt - it seems reasonable that the user might want to talk about multiple things in one chat (even if only because they aren't very good at using the ChatGPT interface), and trying to frame everything as a computer science problem might not be what most people want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452de5db",
   "metadata": {},
   "source": [
    "Then, repeat the experiment using a small model like [SmolLM2-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct). \n",
    "\n",
    "* What differences did you notice between the large and small models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5975afbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 290/290 [00:00<00:00, 823.55it/s, Materializing param=model.norm.weight]                              \n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from accelerate import Accelerator\n",
    "\n",
    "device = Accelerator().device\n",
    "\n",
    "smol = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M-Instruct\", device = device)\n",
    "\n",
    "for test, n in zip([chat_test1, chat_test2, chat_test3], range(1, 4)):\n",
    "    with open(f'{OUTPUTDIR}/smol_test{n}.txt', 'w') as outfile:\n",
    "        outfile.write(f\"Responses to test {n} with SmolLM2\\n\")\n",
    "        for _ in range(3):\n",
    "            outfile.write(f\"{'=' * 40} \\n\\n\")\n",
    "            smol_response = smol(test)\n",
    "            outfile.write(smol_response[0]['generated_text'][-1]['content'] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f30df",
   "metadata": {},
   "source": [
    "### Differences between large and small models\n",
    "* obviously, the small language models are much less sophisticated\n",
    "    * some lovely wisdom: \"the Maillard reaction is a chemical reaction between amino acids and reducing sugars in cooked food, typically involving amino acids and reducing sugars\"\n",
    "* the small LMs also cut off randomly, not sure what that's about - perhaps they have a token limit?\n",
    "* the small LMs also seem to make it much more dramatic, as if they're giving a presentation about it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
