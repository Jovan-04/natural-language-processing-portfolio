{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd54353",
   "metadata": {},
   "source": [
    "## Applied Exploration - Fortnight 1\n",
    "\n",
    "The `roberta-base-go_emotions` model is documented here: https://huggingface.co/SamLowe/roberta-base-go_emotions\n",
    "\n",
    "Answer some questions about this:\n",
    "* What is `roberta-base`? Write down some things you can learn about it from the documentation.\n",
    "* What is `go_emotions`? Write down some things you can learn about it from the documentation.\n",
    "\n",
    "Go to the Hugging Face models page: https://huggingface.co/models\n",
    "* click `Text Classification`\n",
    "* Try some additional models\n",
    "    - test out at least one more sentiment/emotions model\n",
    "    - test out at least two other kinds of models - like news topic classification or spam detection\n",
    "    - write down some info about the models you found\n",
    "        - what is it for?\n",
    "        - who made it?\n",
    "        - what kind of data was it trained on?\n",
    "        - are they based on some other model and trained on new data (*fine-tuned*) for a specific task?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ee673",
   "metadata": {},
   "source": [
    "### roberta-base\n",
    "* pretrained masked language transformer model\n",
    "* from my brief research, masked language models are kind of like a spoon blank in woodcarving; a rough start to whatever kind of NLP task you want to do, but later something that needs to be refined\n",
    "* self-supervised pre-training\n",
    "* I was surprised to see how strong the bias was towards gender stereotypes (the example they gave); I suppose it doesn't surprise me, but that's something to keep in mind\n",
    "\n",
    "### go_emotions\n",
    "* https://huggingface.co/datasets/google-research-datasets/go_emotions\n",
    "* 58,000 Reddit comments labeled with (potentially multiple) emotion categories\n",
    "* data was curated for use in, for example, emotionally-aware chatbots (although the huggingface page doesn't say how they curated it)\n",
    "* I did find it interesting how huggingface has a section on social impact & bias consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcac3a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: psutil in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from accelerate) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.10.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate) (1.3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/evan/evan-drake/cs-195/.venv/lib/python3.12/site-packages (from typer-slim->transformers) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c3804",
   "metadata": {},
   "source": [
    "### tabularisai/multilingual-sentiment-analysis\n",
    "* like roberta, this is a sentiment analysis model; but it's not limited to just English\n",
    "* made by tabularis.ai, a company focused on developing local AI models\n",
    "* trained on only LLM-generated multilingual data\n",
    "* fine-tuned version of `distilbert-base-multilingual-cased`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45db7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 104/104 [00:00<00:00, 415.72it/s, Materializing param=pre_classifier.weight]                                  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'Positive', 'score': 0.3969952166080475},\n",
       " {'label': 'Positive', 'score': 0.7760933041572571},\n",
       " {'label': 'Positive', 'score': 0.8941887021064758},\n",
       " {'label': 'Negative', 'score': 0.6792149543762207}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n",
    "\n",
    "# ; my dog is very pretty; the professor of CS 195 is very intelligent; I don't speak french (google translate)\n",
    "classifier([\"I like Drake University\", \"mi perra es muy muy bonita\", \"el profe de CS 195 es muy inteligente\", \"Je ne parle pas français\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bc9a4",
   "metadata": {},
   "source": [
    "### papluca/xlm-roberta-base-language-detection\n",
    "* also based on roberta; it's interesting to see how the same model can be used for a variety of different kinds of outputs\n",
    "* it looks like it's just made by some guy interested in language detection\n",
    "* trained on 70,000 samples from the [language identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset, which itself looks just like a few different datasets aggregated together\n",
    "* a fine-tuned version of roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1feb513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 658.70it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "XLMRobertaForSequenceClassification LOAD REPORT from: papluca/xlm-roberta-base-language-detection\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'en', 'score': 0.9530580043792725},\n",
       " {'label': 'es', 'score': 0.9931657910346985},\n",
       " {'label': 'es', 'score': 0.9923359751701355},\n",
       " {'label': 'fr', 'score': 0.9915567636489868}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "\n",
    "# ; my dog is very pretty; the professor of CS 195 is very intelligent; I don't speak french (google translate)\n",
    "classifier([\"I like Drake University\", \"mi perra es muy muy bonita\", \"el profe de CS 195 es muy inteligente\", \"Je ne parle pas français\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab66e1c",
   "metadata": {},
   "source": [
    "### mshenoda/roberta-spam\n",
    "* *also* based on roberta! on second thought, maybe I should have gone through to find a different base model\n",
    "* it looks like this guy is a researcher at the University of Drexel with a few AI papers listed on his huggingface progfile\n",
    "* trained on 47,000 labeled ham/spam messages from SMS, Telegram, and Email\n",
    "* fine-tuned from the roberta base again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddbc2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 462.29it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "RobertaForSequenceClassification LOAD REPORT from: mshenoda/roberta-spam\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9999982118606567},\n",
       " {'label': 'LABEL_0', 'score': 0.9999983310699463}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"mshenoda/roberta-spam\")\n",
    "\n",
    "# the hard part about this is that spam filters are good enough now that I hardly get any real spam.\n",
    "# both of these are marked as clean; the first, I pulled from a spammy email (although it might just be annoying marketing)\n",
    "classifier([\"I an reaching out to wish you a very Happy New Year. As we go into this year, are there any upcoming projects that I could assist with in terms of labrotory essential equipment? I would be happy to hop on a call to discuss more. I look forward to hearing from you!\", \n",
    "           \"Hi John, hope you're doing well. Just wanted to wish you a merry christmas, and I hope the kids are doing well!\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
